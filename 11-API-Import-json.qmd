---
title: "API : Importation json"
author: "Claude Grasland"
date: "2025-02-11"
date-format: iso
lang: fr
format:
  html:
    embed-resources: true
    smooth-scroll: true
    fontsize: 0.9em
    toc: true
    toc-depth: 3
    toc-title: "."
    crossrefs-hover: false
    theme: [yeti, style.scss]
execute:
  warning: false
  message: false 
knitr:
  opts_chunk:
    out.width: "100%"
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
---


Le but de ce chapitre n'est pas d'apprendre en détail l'ensemble des possibilités qu'offrent les API pour des utilisateurs avancés, mais de fournir aux étudiants en data mining un certain nombre de solutions simples (mais efficaces) pour extraire des données de façon interactive et assurer leur mise à jour régulière. 



On charge les packages utiles :

```{r}
knitr::opts_chunk$set(echo = TRUE, warning= FALSE, message = FALSE, error=FALSE)

## Affichage de tableaux
library(knitr)

## Requêtes web
library(httr)
library(jsonlite)

## Tidyverse & co
library(dplyr, warn.conflicts = T, quietly = T)
library(ggplot2)
```




## Choix d'une API

La première étape consiste à choisir l'API qui nous intéresse parmi plus de 600. 

### Le site public.opendatasoft


Nous allons centrer notre chapitre sur le site **public.opendatasoft**  qui permet d'accèder à des centaines d'API à l'aide de requêtes normalisées. Sans apprendre en détail le fonctionnement de cette API, on va montrer comment créer de petites fonctions facilitant le travail d'exportation des variables ou des données. 

On peut se rendre sur le site pour parcourir les API proposées en allant à l'adresse :
https://public.opendatasoft.com

```{r, echo=FALSE}
knitr::include_graphics("img/opendatasoft.png")
```

### Catalogue des API

Plutôt que de pacourir le site web, on peut télécharger le catalogue général des bases de données du site public.opendatasoft ... en se servant d'une requête API

```{r}
x<-GET('https://public.opendatasoft.com/api/datasets/1.0/search/?q=&rows=1000&start=0')
y<-fromJSON(rawToChar((x$content)))
cat<-y$datasets$metas
row.names(cat)<-y$datasets$datasetid
kable(head(cat[,c(12,1,6,7,8)]),row.names = T)
```

On a donc récupéré un tableau qui comporte 605 lignes correspondant à 605 bases de données. Le nom des lignes du tableau indique le code de la base de données que l'on va utiliser ensuite dans les requêtes. 

### Choix d'un tableau de données

On suppose que le choix s'est porté sur la base de données dont le nom de code est  *prix-des-carburants-j-1*

```{r, echo=FALSE}
knitr::include_graphics("img/carbu.jpg")
```

L'onglet information nous indique qu'il s'agit d'un site produit par le minstère de l'économie et des finances pour faciliter l'accès en temps réel au prix des carburants dans les stations services. Le but est d'informer les conosmmateurs des stations les moins chères à proximité de son domicile afin de stimuler la concurrence et faire baisser les prix. 

Il est indiqué que la base se limite aux prix des douze derniers mois mais nous avons pu vérifier qu'on trouve en fait des données sur plus de trois ans. 





### Liste des variables

Avant de télécharger les données, on regarde précisément la liste des variables disponibles. On peut le faire sur le site web en parcourant les onglets. Mais il est également possible de lancer une requête pour connaître les variables du tableau que l'on va télécharger ainsi que les variables pouvant servir de "facettes" c'est-à-dire permettant d'effectuer des requêtes.


```{r}
tab<-"prix-des-carburants-j-1"
url<-paste("https://public.opendatasoft.com/api/v2/catalog/datasets/",tab,"?",sep="")
x<-GET(url)
y<-fromJSON(rawToChar(x$content))
var<-y$dataset$fields

head(var)


```


On extrait du tableau les colonnes qui fournissent le nom des variables, leur définition et leur type

```{r}
var <- var  %>% select(name, label, type)
kable(var)
```


On peut transformer le programme que l'on vient d'executer en fonction pour un usage plus simple : 

```{r}
get_variables<-function(idtab = "prix-des-carburants-j-1") {
  url<-paste("https://public.opendatasoft.com/api/v2/catalog/datasets/",idtab,"?",sep="")
  x<-GET(url)
  y<-fromJSON(rawToChar((x$content)))
  var<-y$dataset$fields
  var <- var %>% select(name, label, type)
  return(var)
}
```

On peut désormais appliquer notre fonction sur n'importe quel autre tableau du catalogue. Par exemple, si on choisit le tableau `qualite_de-lair-france` on obtient la liste de variables suivante :

```{r}
var<-get_variables("qualite-de-lair-france")
kable(var)
```



## Récupération des données


Pour des utilisateurs non spécialiste, il est difficile de lancer une requête complexe qui suppose une maîtrise avancée des API et des protocoles de requête SOAP et REST. Nous allons opter ici pour une **stratégie pragmatique** (mais efficace) qui consiste à :

1. Utiliser l'interface public.opendatasoft pour rédiger une requête
2. Récupérer le lien de téléchargement
3. Télécharger les données correspondant à la requête 
4. Effectuer les opérations de nettoyage des données et réaliser un graphique
5. Modifier le lien et effectuer à nouveau le étapes 3 et 4
6. Construire une fonction paramétrique de téléchargement + nettoyage + visualisation ...

Pour illustrer cette stratégie, nous allons essayer de créer dans R une fonction automatisée qui télécharge le prix du carburant d'une commune et produit un graphique montrant son évolution au cours du temps dans les dfférentes stations. Nous allons ainsi essayer de reconstituer une application assez proche de celle du ministère de l'économie intitulée ["essence pas cher"](https://www.essence-pas-cher.fr/).



![Essence pas cher](img/essence-pas-cher.jpg)

Nous ne chercherons toutefois pas à obtenir uniquement le dernier prix en date des stations mais plutôt à voir lesquelles sont les plus ou les mons chers sur une période de quelques années. 

### 1. Rédaction d'une requête sur public opendatasoft


On utilise les filtres de l'interface pour sélectionner la commune cible à l'aide de son code postal (ex. 94370 = Sucy-en-Brie) et du type carburant (ex. Gazole) : 

![Filtres](img/essence-001.jpg)

### 2. Récupération du lien de téléchargement

Une fois terminée la mise en place des filtres, on se déplace vers la fenêtre "**Export**" et on choisit le type de format de sortie que l'on souhaite obtenir. Nous pourrions obtenir des fichiers au format texte (*.csv*) ou tableur (*.xls*) mais nous allons adopter ici le format .json qui est plus universel dans le domaine de la data science et qui simplifie les transferts de données entre utilisateurs de différents langages de programmation tels que R ou Python.

Un click de souris sur le lien nous permet de récupérer l'URL de téléchargement : 

![URL](img/essence-002.jpg)

Même si certains caractères spéciaux sont difficiles à comprendre comme **%3A** ou **%22** on devine assez facilement la fonction des différents segments de la chaine de caractère qui constitue l'URL de requête : 

- adresse du site web opendatasoft : *https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/*
- choix de la base de données : *prix-des-carburants-j-1*
- format d'export et langue : *exports/json?lang=fr*
- selection du carburant : *&refine=fuel%3A%22Gazole%22*
- selection de la commune par son code postal : *&qv1=(94370)*
- fuseau horaire (pour dater la requête) : *&timezone=Europe%2FParis*


### 3. Recupération des données à partir de l'URL

Nous pouvons maintenant rédiger un petit programme très simple qui va récupérer les données à partir de ce lien


```{r}
link<-"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/prix-des-carburants-j-1/exports/json?lang=fr&refine=fuel%3A%22Gazole%22&qv1=(94370)&timezone=Europe%2FParis"
y<-fromJSON(link)
head(y)
```

A la différence de la méthode GET vue au chapitre précédent, nous récupérons directement le fichier de données sans avoir besoin d'effectuer des transformations de type RawToChar. C'est donc beaucoup plus simple mais, en contrepartie, nous perdons toute une série d'informations qu'apportait la procédure dans les règles de l'art (date de téléchargement, messages d'erreur, version des données, etc.). 



### 4. Nettoyage des données


Nous procédons ensuite à un petit nettoyage pour ne garder que les variables utiles :

```{r}
names(y)
don <- y %>% select(name,address, update, price = price_gazole ) %>% 
  mutate(update =as.Date(update)) %>%
  arrange(update)

```

Il y a toutefois une mauvaise surprise ... les données semblent erronées à partir d'une certaine date 

```{r}
ggplot(don) +aes(x=update, y=price, col=address) + geom_point()
```

En fait ... les chiffres qui sont fournis après le 26 mars ont été divisés mystérieusement par 1000. Il faut donc corriger ce problème :

```{r}
library(ggplot2)
don<-don %>%  mutate(price_OK = case_when(price ==0 ~ NA,
                             price < 1 ~ price*1000,
                             TRUE ~ price))
ggplot(don) +aes(x=update, y=price_OK, col=address) + geom_point()
```

On note qu'il este une valeur aberrante mais sinon il est désormais possible de bien suivre l'évolution des prix au cours des trois dernières années et de repérer quelles est la station la moins chèr aux différentes dates.



### 5. Changement de lien

Essayons maintenant de reprendre l'ensemble de notre programme en changeant juste de commune dans le lien initial. On va ici soigner la rédaction du programme car nous comptons ensuite le transformer en fonction

On remplace le code postal de Sucy-en-Brie (94370) par celui d'Ivry-sur-Seine(94200). 


```{r}
# Choix du lien (changement du code postal)
link<-"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/prix-des-carburants-j-1/exports/json?lang=fr&refine=fuel%3A%22Gazole%22&qv1=(94200)&timezone=Europe%2FParis"

# Importation des données
y<-fromJSON(link)

# Selection des variables
don <- y %>% select(name,address, update, price = price_gazole ) %>% 
  mutate(update =as.Date(update)) %>%
  arrange(update)

# Nettoyage des erreurs principales
don<-don %>%  mutate(price_OK = case_when(price ==0 ~ NA,
                             price < 1 ~ price*1000,
                             TRUE ~ price))

# Réalisation d'un graphique
ggplot(don) +aes(x=update, y=price_OK, col=address) + geom_point()

```






### 6. Rédaction d'une Fonction

On peut maintenant écrire une fonction qui ne va dépendre que du code postal et va fournir en sortie le tableau de données. Tout ce que nous avons à faire est de modifier le lien en fonction du code postal qui sera le paramètre de la fonction. 

Pour cela nous utilisons la commande R `pasteO()`qui permet de coller des chaînes de caractères sans ajouter d'expaces. Ici nous recollons le début de l'URL, le code de la commune que nous avons modifié et la fin de l'URL. 



```{r}

gazole_tab <- function(code="94370") { 
# Choix du lien (changement du code postal)
link<-paste0("https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/prix-des-carburants-j-1/exports/json?lang=fr&refine=fuel%3A%22Gazole%22&qv1=(", code,")&timezone=Europe%2FParis")

# Importation des données
y<-fromJSON(link)

# Selection des variables
tab <- y %>% select(name,address, update, price = price_gazole ) %>% 
  mutate(update =as.Date(update)) %>%
  arrange(update)

# Nettoyage des erreurs principales
tab<-tab %>%  mutate(price_OK = case_when(price ==0 ~ NA,
                             price < 1 ~ price*1000,
                             TRUE ~ price))

return(tab)

}


```


Pour tester notre fonction `gazole_tab()`, on prend en exemple une nouvelle commune, par exemple Saint-Maur des Fossés (94100) :

```{r}
res<-gazole_tab("94100")
head(res)
```


Mais on pourrait aussi faire une fonction  `gazole_graph()`qui renvoie non pas le tableau mais le graphique :

```{r}

gazole_graph <- function(code="94370") { 
# Choix du lien (changement du code postal)
link<-paste0("https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/prix-des-carburants-j-1/exports/json?lang=fr&refine=fuel%3A%22Gazole%22&qv1=(", code,")&timezone=Europe%2FParis")

# Importation des données
y<-fromJSON(link)

# Selection des variables
don <- y %>% select(name,address, update, price = price_gazole ) %>% 
  mutate(update =as.Date(update)) %>%
  arrange(update)

# Nettoyage des erreurs principales
don<-don %>%  mutate(price_OK = case_when(price ==0 ~ NA,
                             price < 1 ~ price*1000,
                             TRUE ~ price))

# Réalisation d'un graphique
graph<-ggplot(don) +aes(x=update, y=price_OK, col=address) + geom_point()

return(graph)

}


```



On teste la fonction sur Saint-Maur des Fossés (94100) :

```{r}
gazole_graph("94100")

```

Mais le plus intéressant est de faire une fonction unique `gazole()`qui permet de renvoyer à la fois le tableau et le graphique en indiquant en sortie une **liste** d'objets comprenant à la fois le tableau (objet de type data.frame) et le graphique (objet de type ggplot2). 

```{r}

gazole <- function(code="94370") { 
# Choix du lien (changement du code postal)
link<-paste0("https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/prix-des-carburants-j-1/exports/json?lang=fr&refine=fuel%3A%22Gazole%22&qv1=(", code,")&timezone=Europe%2FParis")

# Importation des données
y<-fromJSON(link)

# Selection des variables
tab <- y %>% select(name,address, update, price = price_gazole ) %>% 
  mutate(update =as.Date(update)) %>%
  arrange(update)

# Nettoyage des erreurs principales
tab<-tab %>%  mutate(price_OK = case_when(price ==0 ~ NA,
                             price < 1 ~ price*1000,
                             TRUE ~ price))

# Réalisation d'un graphique
graph<-ggplot(don) +aes(x=update, y=price_OK, col=address) + geom_point()

return(list("tab"=tab, "graph"=graph))

}


```



Il suffit maintenant d'executer une seule fois la fonction (un seul appel de l'API) pour pouvoir ensuite au choix utiliser le tableau ou afficher le graphique.

```{r}
res<-gazole("94100")
head(res$tab)
res$graph
```


## Conclusion

Ce chapitre a permis de combiner trois apprentissages fondamentaux du data mining  qui seront repris ensuie à plusieurs reprises :

1. **Utiliser des API pour récupérer directement ses données** sans effectuer de téléchargement "à la main".
2. **Nettoyer les données** reçues avant de les utiliser et automatiser autant que possible les procédures de nettoyages.
3. **Créer ses propres fonctions** pour automatiser les tâches de récupération des données, nettoyage et production de tableaux ou graphiques. 









